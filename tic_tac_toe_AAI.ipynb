{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tic_tac_toe_AAI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saurabhburewar/ML_RL-TicTacToe/blob/main/tic_tac_toe_AAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVDSonsiiyTQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "boardDim = (3, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for minimax algorithm"
      ],
      "metadata": {
        "id": "NF-ObdTa18Zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "player1, player2 = 'x', 'o'\n",
        "def isBoardEmpty(board) :\n",
        " \n",
        "    for i in range(3) :\n",
        "        for j in range(3) :\n",
        "            if (board[i][j] == '_') :\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "def isWinner(board) :\n",
        "    for i in range(3) :    \n",
        "        if (board[i][0] == board[i][1] and board[i][1] == board[i][2]) :       \n",
        "            if (board[i][0] == player1) :\n",
        "                return 1\n",
        "            elif (board[i][0] == player2) :\n",
        "                return -1\n",
        " \n",
        "    for j in range(3) :\n",
        "      \n",
        "        if (board[0][j] == board[1][j] and board[1][j] == board[2][j]) :\n",
        "         \n",
        "            if (board[0][j] == player1) :\n",
        "                return 1\n",
        "            elif (board[0][j] == player2) :\n",
        "                return -1\n",
        " \n",
        "    if (board[0][0] == board[1][1] and board[1][1] == board[2][2]) :\n",
        "     \n",
        "        if (board[0][0] == player1) :\n",
        "            return 1\n",
        "        elif (board[0][0] == player2) :\n",
        "            return -1\n",
        " \n",
        "    if (board[0][2] == board[1][1] and board[1][1] == board[2][0]) :\n",
        "     \n",
        "        if (board[0][2] == player1) :\n",
        "            return 1\n",
        "        elif (board[0][2] == player2) :\n",
        "            return -1\n",
        " \n",
        "    return 0\n",
        "\n",
        "def minimaxAlgo(board, layer, flag) :\n",
        "    reward = isWinner(board)\n",
        "    if (reward == 1) :\n",
        "        return reward\n",
        " \n",
        "    if (reward == -1) :\n",
        "        return reward\n",
        "    if (isBoardEmpty(board) == False) :\n",
        "        return 0\n",
        "    if (flag) :    \n",
        "        maxReward = -898\n",
        "        for i in range(3) :        \n",
        "            for j in range(3) :\n",
        "                if (board[i][j]=='_') :\n",
        "                    board[i][j] = player1\n",
        "                    maxReward = max( maxReward, minimaxAlgo(board,\n",
        "                                              layer + 1,\n",
        "                                              not flag) )\n",
        "                    board[i][j] = '_'\n",
        "        return maxReward\n",
        "\n",
        "    else :\n",
        "        maxReward = 898\n",
        " \n",
        "        for i in range(3) :        \n",
        "            for j in range(3) :              \n",
        "                if (board[i][j] == '_') :        \n",
        "                    board[i][j] = player2\n",
        "                    maxReward = min(maxReward, minimaxAlgo(board, layer + 1, not flag))\n",
        " \n",
        "                    board[i][j] = '_'\n",
        "        return maxReward\n",
        " \n",
        "\n",
        "def findOptimalPosition(board) :\n",
        "    optimalVal = -998\n",
        "    optimalMove = (-1, -1)\n",
        "    for i in range(3) :    \n",
        "        for j in range(3) :\n",
        "            if (board[i][j] == '_') :\n",
        "                board[i][j] = player1\n",
        "\n",
        "                moveVal = minimaxAlgo(board, 0, False)\n",
        "                board[i][j] = '_'\n",
        "\n",
        "                if (moveVal > optimalVal) :               \n",
        "                    optimalMove = (i, j)\n",
        "                    optimalVal = moveVal\n",
        "\n",
        "    return optimalMove"
      ],
      "metadata": {
        "id": "5iVxBHeHHmQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computer player"
      ],
      "metadata": {
        "id": "6ajHCfXmrKwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ComputerPlayer:\n",
        "  def __init__(self, playerName, epsilon):\n",
        "    self.playerName = playerName\n",
        "    self.boardState = []\n",
        "    self.learningRate = 0.2\n",
        "    self.epsilon = epsilon\n",
        "    self.Y = 0.9\n",
        "    self.policy = {}\n",
        "\n",
        "  def hashedBoard(self, board):\n",
        "    return str(board.reshape(boardDim[1]*boardDim[0]))\n",
        "\n",
        "  def updatePolicy(self, utility):\n",
        "    for state in reversed(self.boardState):\n",
        "      if self.policy.get(state) is None:\n",
        "        self.policy[state] = 0\n",
        "      self.policy[state] += self.learningRate * (self.Y * utility - self.policy[state])\n",
        "      utility = self.policy[state]\n",
        "\n",
        "\n",
        "  def findNextAction(self, emptyPositions, currentState, playerSymbol):\n",
        "    if np.random.uniform(0, 1) <= self.epsilon:\n",
        "      idx = np.random.choice(len(emptyPositions))\n",
        "      actionToTake = emptyPositions[idx]\n",
        "    else:\n",
        "      maxValue = -898\n",
        "      for temp in emptyPositions:\n",
        "        nextBoardState = currentState.copy()\n",
        "        nextBoardState[temp] = playerSymbol\n",
        "        nextBoardStatus = self.hashedBoard(nextBoardState)\n",
        "        if self.policy.get(nextBoardStatus) is None:\n",
        "          value = 0\n",
        "        else:\n",
        "          self.policy.get(nextBoardStatus)\n",
        "        \n",
        "        if value >= maxValue:\n",
        "          maxValue = value\n",
        "          actionToTake = temp\n",
        "\n",
        "    return actionToTake  \n",
        "\n",
        "  def addMove(self, state):\n",
        "    self.boardState.append(state)\n",
        "\n",
        "  def reset(self):\n",
        "    self.boardState = []\n",
        "\n",
        "\n",
        "  def savePolicy(self):\n",
        "    file = open('policy_' + str(self.playerName), 'wb')\n",
        "    pickle.dump(self.policy, file)\n",
        "    file.close()\n",
        "\n",
        "  def loadPolicy(self, file):\n",
        "    file = open(file, 'rb')\n",
        "    self.policy = pickle.load(file)\n",
        "    file.close()"
      ],
      "metadata": {
        "id": "cveD25IDukGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "computer player with minimax"
      ],
      "metadata": {
        "id": "5IS-_xL42Cwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ComputerPlayer2:\n",
        "  def __init__(self, playerName, epsilon):\n",
        "    self.playerName = playerName\n",
        "    self.boardState = []\n",
        "    self.learningRate = 0.2\n",
        "    self.epsilon = epsilon\n",
        "    self.Y = 0.9\n",
        "    self.policy = {}\n",
        "\n",
        "  def hashedBoard(self, board):\n",
        "    hashedState = str(board.reshape(boardDim[1]*boardDim[0]))\n",
        "    return hashedState\n",
        "\n",
        "# q-learning updation\n",
        "  def updatePolicy(self, utility):\n",
        "    for state in reversed(self.boardState):\n",
        "      if self.policy.get(state) is None:\n",
        "        self.policy[state] = 0\n",
        "      self.policy[state] += self.learningRate * (self.Y * utility - self.policy[state])\n",
        "      utility = self.policy[state]\n",
        "\n",
        "  def findNextAction(self, emptyPositions, currentState, playerSymbol):\n",
        "    if np.random.uniform(0, 1) <= self.epsilon:\n",
        "      idx = np.random.choice(len(emptyPositions))\n",
        "      actionToTake = emptyPositions[idx]\n",
        "    else:\n",
        "      currentState1 = np.full(boardDim, '_')\n",
        "      for i in range(boardDim[0]):\n",
        "        for j in range(boardDim[1]):\n",
        "          if currentState[i][j] == 1:\n",
        "            currentState1[i][j] = 'x'\n",
        "          if currentState[i][j] == -1:\n",
        "            currentState1[i][j] = 'o'\n",
        "      actionToTake = findOptimalPosition(currentState1)\n",
        "      \n",
        "    return actionToTake  \n",
        "\n",
        "  def addMove(self, state):\n",
        "    self.boardState.append(state)\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self.boardState = []\n",
        "\n",
        "  def savePolicy(self):\n",
        "    file = open('policy_' + str(self.playerName), 'wb')\n",
        "    pickle.dump(self.policy, file)\n",
        "    file.close()\n",
        "\n",
        "  def loadPolicy(self, file):\n",
        "    file = open(file, 'rb')\n",
        "    self.policy = pickle.load(file)\n",
        "    file.close()"
      ],
      "metadata": {
        "id": "Kh6whPxSH-k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class ComputerPlayer:\n",
        "#   def __init__(self, playerName, epsilon):\n",
        "#     self.playerName = playerName\n",
        "#     self.boardState = []\n",
        "#     self.learningRate = 0.2\n",
        "#     self.epsilon = epsilon\n",
        "#     self.Y = 0.9\n",
        "#     self.policy = {}\n",
        "\n",
        "#   def hashedBoard(self, board):\n",
        "#     hashedState = str(board.reshape(boardDim[1]*boardDim[0]))\n",
        "#     return hashedState\n",
        "\n",
        "\n",
        "\n",
        "#   def updatePolicy(self, utility):\n",
        "#     for state in reversed(self.boardState):\n",
        "#       if self.policy.get(state) is None:\n",
        "#         self.policy[state] = 0\n",
        "#       self.policy[state] += self.learningRate * (self.Y * utility - self.policy[state])\n",
        "#       utility = self.policy[state]\n",
        "\n",
        "\n",
        "#   def findNextAction(self, emptyPositions, currentState, playerSymbol):\n",
        "#     # print(\"Player Symbol: \", playerSymbol)\n",
        "#     if np.random.uniform(0, 1) <= self.epsilon:\n",
        "#       idx = np.random.choice(len(emptyPositions))\n",
        "#       actionToTake = emptyPositions[idx]\n",
        "#     else:\n",
        "#       opp_playerSymbol = -playerSymbol\n",
        "#       opp_maxValue = -888\n",
        "#       for temp in emptyPositions:\n",
        "#         opp_nextBoardState = currentState.copy()\n",
        "#         opp_nextBoardState[temp] = opp_playerSymbol\n",
        "#         opp_nextBoardStatus = self.hashedBoard(opp_nextBoardState)\n",
        "#         if self.policy.get(opp_nextBoardStatus) is None:\n",
        "#           opp_value = 0\n",
        "#         else:\n",
        "#           self.policy.get(opp_nextBoardStatus)\n",
        "        \n",
        "#         if opp_value >= opp_maxValue:\n",
        "#           opp_maxValue = opp_value\n",
        "#           opp_actionToTake = temp\n",
        "      \n",
        "#       # print(\"Opp: \", opp_actionToTake)\n",
        "\n",
        "#       maxValue = -999\n",
        "#       for temp in emptyPositions:\n",
        "#         nextBoardState = currentState.copy()\n",
        "#         nextBoardState[temp] = playerSymbol\n",
        "#         nextBoardStatus = self.hashedBoard(nextBoardState)\n",
        "#         if self.policy.get(nextBoardStatus) is None:\n",
        "#           value = 0\n",
        "#         else:\n",
        "#           self.policy.get(nextBoardStatus)\n",
        "        \n",
        "#         if value >= maxValue:\n",
        "#           maxValue = value\n",
        "#           actionToTake = temp\n",
        "      \n",
        "#       # print(\"Curr: \", actionToTake)\n",
        "    \n",
        "#       if opp_maxValue >= maxValue:\n",
        "#         maxValue = opp_maxValue\n",
        "#         actionToTake = opp_actionToTake\n",
        "\n",
        "#       # print(\"Opt: \", actionToTake)\n",
        "\n",
        "#     return actionToTake  \n",
        "\n",
        "#   def addMove(self, state):\n",
        "#     self.boardState.append(state)\n",
        "\n",
        "#   def reset(self):\n",
        "#     self.boardState = []\n",
        "\n",
        "#   def savePolicy(self):\n",
        "#     print(self.policy)\n",
        "#     file = open('policy_' + str(self.playerName), 'wb')\n",
        "#     pickle.dump(self.policy, file)\n",
        "#     file.close()\n",
        "\n",
        "#   def loadPolicy(self, file):\n",
        "#     file = open(file, 'rb')\n",
        "#     self.policy = pickle.load(file)\n",
        "#     file.close()"
      ],
      "metadata": {
        "id": "k9NTzbrkBB6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Human player"
      ],
      "metadata": {
        "id": "kmajaoA5rlaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HumanPlayer:\n",
        "  def __init__(self, playerName):\n",
        "    self.playerName = playerName\n",
        "\n",
        "  def findNextAction(self, emptyPositions):\n",
        "    while True:\n",
        "      i = int(input(\"Input next row: \"))\n",
        "      j = int(input(\"Input next column: \"))\n",
        "      actionToTake = (i, j)\n",
        "      if actionToTake in emptyPositions:\n",
        "        return actionToTake"
      ],
      "metadata": {
        "id": "qz0ZqARrpTOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "State of the board"
      ],
      "metadata": {
        "id": "XCIFlrgTr7na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class State:\n",
        "  def __init__(self, player1, player2):\n",
        "    self.state = np.zeros(boardDim)\n",
        "    self.player1 = player1\n",
        "    self.player2 = player2\n",
        "    self.gameOver = False\n",
        "    self.hashedState = None\n",
        "    self.playerSymbol = 1\n",
        "\n",
        "  \n",
        "  def hashedBoard(self):\n",
        "    self.hashedState = str(self.state.reshape(boardDim[1]*boardDim[0]))\n",
        "    return self.hashedState\n",
        "\n",
        "  def validPosition(self):\n",
        "    emptyPositions = []\n",
        "    for i in range(boardDim[0]):\n",
        "      for j in range(boardDim[1]):\n",
        "        if self.state[i, j] == 0:\n",
        "          emptyPositions.append((i, j))\n",
        "    return emptyPositions\n",
        "\n",
        "  def updateVariables(self, temp):\n",
        "    self.state[temp] = self.playerSymbol\n",
        "    if self.playerSymbol == 1:\n",
        "      self.playerSymbol = -1\n",
        "    else:\n",
        "      self.playerSymbol = 1\n",
        "\n",
        "  def utility(self):\n",
        "    result = self.winner()\n",
        "    if result == 1:\n",
        "      self.player1.updatePolicy(1)\n",
        "      self.player2.updatePolicy(0)\n",
        "    elif result == -1:\n",
        "      self.player1.updatePolicy(0)\n",
        "      self.player2.updatePolicy(1)\n",
        "    else:\n",
        "      self.player1.updatePolicy(0.1)\n",
        "      self.player2.updatePolicy(0.5)\n",
        "\n",
        "  def winner(self):\n",
        "    for i in range(boardDim[0]):\n",
        "      if sum(self.state[i, :]) == 3:\n",
        "        self.gameOver = True\n",
        "        return 1\n",
        "      if sum(self.state[i, :]) == -3:\n",
        "        self.gameOver = True\n",
        "        return -1\n",
        "\n",
        "    for i in range(boardDim[1]):\n",
        "      if sum(self.state[:, i]) == 3:\n",
        "        self.gameOver = True\n",
        "        return 1\n",
        "      if sum(self.state[:, i]) == -3:\n",
        "        self.gameOver = True\n",
        "        return -1\n",
        "\n",
        "    sumDiag1 = 0\n",
        "    sumDiag2 = 0\n",
        "    sumDiag = 0\n",
        "    for i in range(boardDim[1]):\n",
        "      sumDiag1 += self.state[i, i]\n",
        "      sumDiag2 += self.state[i, boardDim[1]-i-1]\n",
        "      if sumDiag < abs(sumDiag1):\n",
        "        sumDiag = sumDiag1\n",
        "      if sumDiag < abs(sumDiag2):\n",
        "        sumDiag = sumDiag2\n",
        "\n",
        "    if sumDiag == 3:\n",
        "      self.gameOver = True\n",
        "      return 1\n",
        "    if sumDiag == -3:\n",
        "      self.gameOver = True\n",
        "      return -1\n",
        "\n",
        "    if len(self.validPosition()) == 0:\n",
        "      self.gameOver = True\n",
        "      return 0\n",
        "\n",
        "    self.gameOver = False\n",
        "\n",
        "    return None\n",
        "  \n",
        "  def trainRLagent(self):\n",
        "    for i in range(200000):\n",
        "      if i % 1000 == 0:\n",
        "        print(\"Rounds {}\".format(i))\n",
        "      while not self.gameOver:\n",
        "        emptyPositions = self.validPosition()\n",
        "        player1act = self.player1.findNextAction(emptyPositions, self.state, self.playerSymbol)\n",
        "\n",
        "        self.updateVariables(player1act)\n",
        "        hashedState = self.hashedBoard()\n",
        "        self.player1.addMove(hashedState)\n",
        "        winner = self.winner()\n",
        "        if winner is not None:\n",
        "          self.utility()\n",
        "          self.player1.reset()\n",
        "          self.player2.reset()\n",
        "          self.reset()\n",
        "          break\n",
        "\n",
        "        else:\n",
        "          emptyPositions = self.validPosition()\n",
        "          player2act = self.player2.findNextAction(emptyPositions, self.state, self.playerSymbol)\n",
        "          \n",
        "          self.updateVariables(player2act)\n",
        "          hashedState = self.hashedBoard()\n",
        "          self.player2.addMove(hashedState)\n",
        "          winner = self.winner()\n",
        "          if winner is not None:\n",
        "            self.utility()\n",
        "            self.player1.reset()\n",
        "            self.player2.reset()\n",
        "            self.reset()\n",
        "            break\n",
        "\n",
        "  def playWithRLagent(self):\n",
        "    while not self.gameOver:\n",
        "      emptyPositions = self.validPosition()\n",
        "      player1act = self.player1.findNextAction(emptyPositions, self.state, self.playerSymbol)\n",
        "\n",
        "      self.updateVariables(player1act)\n",
        "      self.drawBoard()\n",
        "      winner = self.winner()\n",
        "      if winner is not None:\n",
        "        if winner == 1:\n",
        "          print(self.player1.playerName, \"Wins!\")\n",
        "        else:\n",
        "          print(\"Match is Draw!\")\n",
        "        \n",
        "        self.reset()\n",
        "        break\n",
        "\n",
        "      else:\n",
        "        emptyPositions = self.validPosition()\n",
        "        player2act = self.player2.findNextAction(emptyPositions)\n",
        "\n",
        "        self.updateVariables(player2act)\n",
        "        self.drawBoard()\n",
        "        winner = self.winner()\n",
        "        if winner is not None:\n",
        "          if winner == -1:\n",
        "            print(self.player2.playerName, \"Wins!\")\n",
        "          else:\n",
        "            print(\"Draw!\")\n",
        "          \n",
        "          self.reset()\n",
        "          break\n",
        "  \n",
        "  def playWithHuman(self):\n",
        "    while not self.gameOver:\n",
        "      emptyPositions = self.validPosition()\n",
        "      print(\"Player 1:\")\n",
        "      player1act = self.player1.findNextAction(emptyPositions)\n",
        "\n",
        "      self.updateVariables(player1act)\n",
        "      self.drawBoard()\n",
        "      winner = self.winner()\n",
        "      if winner is not None:\n",
        "        if winner == 1:\n",
        "          print(self.player1.playerName, \"Wins!\")\n",
        "        else:\n",
        "          print(\"Match is Draw!\")\n",
        "        \n",
        "        self.reset()\n",
        "        break\n",
        "\n",
        "      else:\n",
        "        emptyPositions = self.validPosition()\n",
        "        print(\"PLayer 2:\")\n",
        "        player2act = self.player2.findNextAction(emptyPositions)\n",
        "\n",
        "        self.updateVariables(player2act)\n",
        "        self.drawBoard()\n",
        "        winner = self.winner()\n",
        "        if winner is not None:\n",
        "          if winner == -1:\n",
        "            print(self.player2.playerName, \"Wins!\")\n",
        "          else:\n",
        "            print(\"Match is Draw!\")\n",
        "          \n",
        "          self.reset()\n",
        "          break\n",
        "\n",
        "  def reset(self):\n",
        "    self.state = np.zeros(boardDim)\n",
        "    self.gameOver = False\n",
        "    self.hashedState = None\n",
        "    self.playerSymbol = 1\n",
        "\n",
        "  def drawBoard(self):\n",
        "    for i in range(0, boardDim[0]):\n",
        "      print(\"--------------\")\n",
        "      out = \"| \"\n",
        "      for j in range(0, boardDim[1]):\n",
        "        if self.state[i, j] == 1:\n",
        "          symbol = 'X'\n",
        "        if self.state[i, j] == -1:\n",
        "          symbol = 'O'\n",
        "        if self.state[i, j] == 0:\n",
        "          symbol = ' '\n",
        "        out += symbol + \" | \"\n",
        "      print(out)\n",
        "    print(\"--------------\")"
      ],
      "metadata": {
        "id": "LRuM9Fpoua43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "aD3oftdesErQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training: RL Agent vs RL Agent\n",
        "\n",
        "player1 = ComputerPlayer(\"player1\", 0.3)\n",
        "player2 = ComputerPlayer(\"player2\", 0.3)\n",
        "\n",
        "st = State(player1, player2)\n",
        "print(\"Training...\")\n",
        "st.trainRLagent()\n",
        "\n",
        "player1.savePolicy()\n",
        "player2.savePolicy()"
      ],
      "metadata": {
        "id": "wbcu6QB3up-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training: RL Agent vs Minimax player\n",
        "\n",
        "player1 = ComputerPlayer(\"player1\", 0.3)\n",
        "player2 = ComputerPlayer2(\"player2\", 0.5)\n",
        "\n",
        "st = State(player1, player2)\n",
        "print(\"Training...\")\n",
        "st.trainRLagent()\n",
        "\n",
        "player1.savePolicy()\n",
        "player2.savePolicy()"
      ],
      "metadata": {
        "id": "u0ERh0crJKVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Human vs Computer"
      ],
      "metadata": {
        "id": "dpeoXtqIsHoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "player1 = ComputerPlayer(\"Computer\", 0)\n",
        "player1.loadPolicy(\"policy_player1\")\n",
        "\n",
        "player2 = HumanPlayer(\"You\")\n",
        "\n",
        "st = State(player1, player2)\n",
        "st.playWithRLagent()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QPqnSzn1gJi",
        "outputId": "c47259c6-b858-46a0-cb20-90ab119ae76c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------\n",
            "|   |   |   | \n",
            "--------------\n",
            "|   |   |   | \n",
            "--------------\n",
            "|   |   | X | \n",
            "--------------\n",
            "Input action row: 0\n",
            "Input action col: 0\n",
            "--------------\n",
            "| O |   |   | \n",
            "--------------\n",
            "|   |   |   | \n",
            "--------------\n",
            "|   |   | X | \n",
            "--------------\n",
            "--------------\n",
            "| O |   |   | \n",
            "--------------\n",
            "|   |   |   | \n",
            "--------------\n",
            "|   | X | X | \n",
            "--------------\n",
            "Input action row: 2\n",
            "Input action col: 0\n",
            "--------------\n",
            "| O |   |   | \n",
            "--------------\n",
            "|   |   |   | \n",
            "--------------\n",
            "| O | X | X | \n",
            "--------------\n",
            "--------------\n",
            "| O |   |   | \n",
            "--------------\n",
            "|   |   | X | \n",
            "--------------\n",
            "| O | X | X | \n",
            "--------------\n",
            "Input action row: 0\n",
            "Input action col: 2\n",
            "--------------\n",
            "| O |   | O | \n",
            "--------------\n",
            "|   |   | X | \n",
            "--------------\n",
            "| O | X | X | \n",
            "--------------\n",
            "--------------\n",
            "| O |   | O | \n",
            "--------------\n",
            "|   | X | X | \n",
            "--------------\n",
            "| O | X | X | \n",
            "--------------\n",
            "Input action row: 1\n",
            "Input action col: 2\n",
            "Input action row: 0\n",
            "Input action col: 1\n",
            "--------------\n",
            "| O | O | O | \n",
            "--------------\n",
            "|   | X | X | \n",
            "--------------\n",
            "| O | X | X | \n",
            "--------------\n",
            "You Win!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Human vs Human"
      ],
      "metadata": {
        "id": "yrmR8ZGBsKcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "player1 = HumanPlayer(\"player1\")\n",
        "player2 = HumanPlayer(\"player2\")\n",
        "\n",
        "st = State(player1, player2)\n",
        "st.playWithHuman()"
      ],
      "metadata": {
        "id": "g868w5uUCqiG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}